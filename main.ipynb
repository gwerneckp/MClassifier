{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "We first import `nbtschematic` to be able to load `.schematic` files. It uses `nbtlib` to parse the nbt file and some classes inherented from `numpy` classes to store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SchematicFile 'Schematic': Schematic({'Blocks': ByteArray([Byte(0), Byte(-97), Byte(0), Byte(-97), Byte(-97), Byte(-97), Byte(0), Byte(-97), Byte(0), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(-97), Byte(0), Byte(-97), Byte(0), Byte(-97), Byte(-97), Byte(-97), Byte(0), Byte(-97), Byte(0), Byte(0), Byte(0), Byte(0), Byte(0), Byte(35), Byte(0), Byte(0), Byte(0), Byte(0)]), 'Materials': String('Alpha'), 'Data': ByteArray([Byte(0), Byte(14), Byte(0), Byte(14), Byte(14), Byte(14), Byte(0), Byte(14), Byte(0), Byte(14), Byte(14), Byte(14), Byte(14), Byte(14), Byte(14), Byte(14), Byte(14), Byte(14), Byte(0), Byte(14), Byte(0), Byte(14), Byte(14), Byte(14), Byte(0), Byte(14), Byte(0), Byte(0), Byte(0), Byte(0), Byte(0), Byte(13), Byte(0), Byte(0), Byte(0), Byte(0)]), 'TileEntities': List[BlockEntity]([]), 'Entities': List[Entity]([]), 'Length': Short(3), 'WEOffsetX': Int(0), 'WEOffsetY': Int(-2), 'WEOriginZ': Int(26), 'WEOffsetZ': Int(-2), 'Height': Short(4), 'WEOriginY': Int(12), 'WEOriginX': Int(-11), 'Width': Short(3)})>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbtschematic import SchematicFile\n",
    "sf = SchematicFile.load(\"schematics/apple.schematic\")\n",
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "What matters to us is the `blocks` property of the `SchematicFile` object. It is a 3D array of `Block` objects. Each `Block` object is a Byte type holding the block id. To be able to use it in our model, we need to convert it into a numpy `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, -97,   0],\n",
       "        [-97, -97, -97],\n",
       "        [  0, -97,   0]],\n",
       "\n",
       "       [[-97, -97, -97],\n",
       "        [-97, -97, -97],\n",
       "        [-97, -97, -97]],\n",
       "\n",
       "       [[  0, -97,   0],\n",
       "        [-97, -97, -97],\n",
       "        [  0, -97,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,  35,   0],\n",
       "        [  0,   0,   0]]], dtype=int8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.asarray(sf.blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - cube with 9^3 blocks and 16 block types\n",
    "\n",
    "The first model is a simple cube with 9^3 blocks and 16 block types. \n",
    "\n",
    "**Objective:** Get the model to classify the blocks correctly, we will only provide schematics of cubes with ONLY 1 block type, but in random positions (We will basically fill the cube with 1 block type, and then remove some blocks randomly).\n",
    "\n",
    "**Why:** This is to make sure we are able to train the model to classify the blocks correctly, then we will move on to more complex models where it will actually be classifying multiple block structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid blocks\n",
    "\n",
    "We will only be using the following blocks:\n",
    "**Air, Dirt, Oak Log, Oak Leaves, Stone Brick, Cobblestone, Glass, Sandstone, Redstone Lamp, Iron Bars, Stone Brick, Bricks, Block of Quartz, White Wool, Bookshelf, White Terracotta, Nether Brick**\n",
    "\n",
    "\n",
    "I already have a schematic file with all these blocks lined up in a row, so we will just load that and use it to get the block ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7941f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7941f_level0_col0\" class=\"col_heading level0 col0\" >Label</th>\n",
       "      <th id=\"T_7941f_level0_col1\" class=\"col_heading level0 col1\" >Block ID</th>\n",
       "      <th id=\"T_7941f_level0_col2\" class=\"col_heading level0 col2\" >Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row0_col0\" class=\"data row0 col0\" >Air</td>\n",
       "      <td id=\"T_7941f_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_7941f_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row1_col0\" class=\"data row1 col0\" >Dirt</td>\n",
       "      <td id=\"T_7941f_row1_col1\" class=\"data row1 col1\" >3</td>\n",
       "      <td id=\"T_7941f_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row2_col0\" class=\"data row2 col0\" >Oak Log</td>\n",
       "      <td id=\"T_7941f_row2_col1\" class=\"data row2 col1\" >17</td>\n",
       "      <td id=\"T_7941f_row2_col2\" class=\"data row2 col2\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row3_col0\" class=\"data row3 col0\" >Oak Leaves</td>\n",
       "      <td id=\"T_7941f_row3_col1\" class=\"data row3 col1\" >18</td>\n",
       "      <td id=\"T_7941f_row3_col2\" class=\"data row3 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row4_col0\" class=\"data row4 col0\" >Stone Brick</td>\n",
       "      <td id=\"T_7941f_row4_col1\" class=\"data row4 col1\" >97</td>\n",
       "      <td id=\"T_7941f_row4_col2\" class=\"data row4 col2\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row5_col0\" class=\"data row5 col0\" >Cobblestone</td>\n",
       "      <td id=\"T_7941f_row5_col1\" class=\"data row5 col1\" >4</td>\n",
       "      <td id=\"T_7941f_row5_col2\" class=\"data row5 col2\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row6_col0\" class=\"data row6 col0\" >Glass</td>\n",
       "      <td id=\"T_7941f_row6_col1\" class=\"data row6 col1\" >20</td>\n",
       "      <td id=\"T_7941f_row6_col2\" class=\"data row6 col2\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row7_col0\" class=\"data row7 col0\" >Sandstone</td>\n",
       "      <td id=\"T_7941f_row7_col1\" class=\"data row7 col1\" >24</td>\n",
       "      <td id=\"T_7941f_row7_col2\" class=\"data row7 col2\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row8_col0\" class=\"data row8 col0\" >Redstone Lamp</td>\n",
       "      <td id=\"T_7941f_row8_col1\" class=\"data row8 col1\" >123</td>\n",
       "      <td id=\"T_7941f_row8_col2\" class=\"data row8 col2\" >8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row9_col0\" class=\"data row9 col0\" >Iron Bars</td>\n",
       "      <td id=\"T_7941f_row9_col1\" class=\"data row9 col1\" >101</td>\n",
       "      <td id=\"T_7941f_row9_col2\" class=\"data row9 col2\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row10_col0\" class=\"data row10 col0\" >Bricks</td>\n",
       "      <td id=\"T_7941f_row10_col1\" class=\"data row10 col1\" >45</td>\n",
       "      <td id=\"T_7941f_row10_col2\" class=\"data row10 col2\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row11_col0\" class=\"data row11 col0\" >Block of Quartz</td>\n",
       "      <td id=\"T_7941f_row11_col1\" class=\"data row11 col1\" >-101</td>\n",
       "      <td id=\"T_7941f_row11_col2\" class=\"data row11 col2\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row12_col0\" class=\"data row12 col0\" >White Wool</td>\n",
       "      <td id=\"T_7941f_row12_col1\" class=\"data row12 col1\" >35</td>\n",
       "      <td id=\"T_7941f_row12_col2\" class=\"data row12 col2\" >12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row13_col0\" class=\"data row13 col0\" >Bookshelf</td>\n",
       "      <td id=\"T_7941f_row13_col1\" class=\"data row13 col1\" >47</td>\n",
       "      <td id=\"T_7941f_row13_col2\" class=\"data row13 col2\" >13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row14_col0\" class=\"data row14 col0\" >White Terracotta</td>\n",
       "      <td id=\"T_7941f_row14_col1\" class=\"data row14 col1\" >-97</td>\n",
       "      <td id=\"T_7941f_row14_col2\" class=\"data row14 col2\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7941f_row15_col0\" class=\"data row15 col0\" >Nether Brick</td>\n",
       "      <td id=\"T_7941f_row15_col1\" class=\"data row15 col1\" >112</td>\n",
       "      <td id=\"T_7941f_row15_col2\" class=\"data row15 col2\" >15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fec4e8eb310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_blocks_schematic = SchematicFile.load(\"schematics/allblocks.schematic\")\n",
    "\n",
    "# We reverse the array because I want the air block to be in the front, it doesn't really matter though\n",
    "blocks = np.asarray(all_blocks_schematic.blocks).flatten()[::-1]\n",
    "\n",
    "# Labels are in the same order as the blocks\n",
    "labels = np.array(['Air', 'Dirt', 'Oak Log', 'Oak Leaves', 'Stone Brick', 'Cobblestone', 'Glass', 'Sandstone', 'Redstone Lamp', 'Iron Bars', 'Bricks', 'Block of Quartz', 'White Wool', 'Bookshelf', 'White Terracotta', 'Nether Brick'])\n",
    "\n",
    "# Create a table with the labels (block names), block ids and index in the array\n",
    "table = np.array([labels, blocks, np.arange(len(labels))]).T\n",
    "\n",
    "# print the table using pandas\n",
    "pd.DataFrame(table, columns=['Label', 'Block ID', 'Index']).style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding the blocks\n",
    "\n",
    "We will use a one hot encoding to represent the blocks. We will have a 16 length vector, with each index representing a block type. The index of the block type will be set to 1, and the rest will be 0.\n",
    "\n",
    "**Why:** Block types is a categorical variable, and we need to represent it in a way that the model can understand. One hot encoding is a good way to do this. If we were to use a simple integer encoding, the model would think that the block types are ordinal, and that the block type with the highest integer is the best block type. This is not the case, so we use one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the blocks to one hot encoding\n",
    "\n",
    "We will use the `block_ids` array to convert the blocks to one hot encoding. We will define a function `one_hot` to create a one hot encoded array and a `convert_blocks_to_one_hot` that takes an id an a dictionnary mapping the block ids to the one hot encoding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(lenght: int, index: int)->np.array:\n",
    "    \"\"\"\n",
    "    Creates a one hot array of the given lenght and sets the index to 1\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((lenght))\n",
    "    one_hot[index] = 1\n",
    "    return one_hot\n",
    "\n",
    "def convert_blocks_to_one_hot(block_id: int, possible_blocks: dict)->np.array:\n",
    "    \"\"\"\n",
    "    Converts an array of block ids to a one hot array\n",
    "    \"\"\"\n",
    "    return one_hot(len(possible_blocks), possible_blocks.get(block_id) or 0)\n",
    "\n",
    "# We need to convert the block ids array to a dictionary of block ids and their index in the array\n",
    "blocks_dict:dict = {block_id: index for index, block_id in enumerate(blocks)}\n",
    "\n",
    "# convert_blocks_to_one_hot(3, blocks_dict)\n",
    "one_hot(16, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "For other models, the dataset will be created inside Minecraft and exported as a `.schematic` file. For this model, we will create the dataset in python. We will randomly set blocks in the cube to a block type with the `random` module. Remember, in this model, **we are only training the model to classify the blocks correctly**, so we will only use 1 block type per cube. This should be a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomCube{count={1: 14, 0: 13}, data=[[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
       "\n",
       "\n",
       " [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
       "\n",
       "\n",
       " [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], target=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "dataset = []\n",
    "\n",
    "class RandomCube:\n",
    "    def __init__(self, block_index: int, dimensions: int=3, air_frequency=0.25):\n",
    "        self.block_index = block_index\n",
    "        self.dimensions = dimensions\n",
    "        self.air_frequency = air_frequency\n",
    "        self.data:np.array = []\n",
    "        self.target:np.array = []\n",
    "\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "        for x in range(self.dimensions):\n",
    "            self.data.append([])\n",
    "            for y in range(self.dimensions):\n",
    "                self.data[x].append([])\n",
    "                for z in range(self.dimensions):\n",
    "                    if random() > self.air_frequency:\n",
    "                        self.data[x][y].append(one_hot(16, self.block_index))\n",
    "                    else:\n",
    "                        self.data[x][y].append(one_hot(16, 0))\n",
    "\n",
    "        self.data = np.array(self.data)\n",
    "        self.target = one_hot(16, self.block_index)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def count(self: int)->int:\n",
    "        # Count each block and return a dict\n",
    "        block_count = {}\n",
    "\n",
    "        for x in self.data:\n",
    "            for y in x:\n",
    "               for z in y:\n",
    "                   for i, is_there in enumerate(z):\n",
    "                        if is_there == 1:\n",
    "                            block_count[i] = block_count.get(i, 0) + 1\n",
    "\n",
    "        return block_count\n",
    "                           \n",
    "    def __repr__(self):\n",
    "        return f\"RandomCube{{count={self.count}, data={self.data}, target={self.target}}}\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"RandomCube{{count={self.count}, data={self.data}, target={self.target}}}\"\n",
    "    \n",
    "    def to_torch(self):\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.target = torch.tensor(self.target, dtype=torch.float32)\n",
    "\n",
    "        return self\n",
    "\n",
    "RandomCube(block_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this RandomCube class, we can create a dataset about 70000 cubes. We will use 50000 for training, 20000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 70000 cubes, each cube has a size of (3, 3, 3, 16) and a target of (16,)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def generate_dataset(size: int=1000, dimensions: int=3, air_frequency: float=0.25)->List[RandomCube]:\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(size):\n",
    "        dataset.append(RandomCube(block_index=randint(0, 15), dimensions=dimensions, air_frequency=air_frequency))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = generate_dataset(size=70000, dimensions=3, air_frequency=0.25)\n",
    "print(f\"Generated {len(dataset)} cubes, each cube has a size of {dataset[0].data.shape} and a target of {dataset[0].target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "It is a 4D model as we have 4 dimensions: x, y, z, and block type. For this simple model, we will use a simple NN with 2 hidden layers. We will use the `relu` activation function for the hidden layers, and `softmax` for the output layer. We will use the `categorical_crossentropy` loss function, and the `adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BlockClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(432, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.flatten()\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the blocks\n",
    "\n",
    "Let's create a random tensor and pass it through the model to see what it predicts. It will give a random result as the model is not trained yet. The weights are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Bricks ~ 10.92%\n",
      "2. Sandstone ~ 8.36%\n",
      "3. Glass ~ 4.76%\n",
      "4. Dirt ~ 3.82%\n",
      "5. Air ~ -0.18%\n",
      "6. Oak Log ~ -0.19%\n",
      "7. Redstone Lamp ~ -0.22%\n",
      "8. Bookshelf ~ -0.46%\n",
      "9. Oak Leaves ~ -0.61%\n",
      "10. Nether Brick ~ -0.99%\n",
      "11. White Terracotta ~ -1.58%\n",
      "12. Block of Quartz ~ -5.57%\n",
      "13. Cobblestone ~ -6.20%\n",
      "14. White Wool ~ -6.43%\n",
      "15. Iron Bars ~ -16.22%\n",
      "16. Stone Brick ~ -23.87%\n"
     ]
    }
   ],
   "source": [
    "model_1 = BlockClassifier().to(\"cuda\")\n",
    "random_prediction = torch.rand(3,3,3,16)\n",
    "prediction = model_1(random_prediction.cuda())\n",
    "\n",
    "prediction = torch.sort(prediction, descending=True)\n",
    "\n",
    "for index, value in enumerate(prediction.indices):\n",
    "    print(f\"{index+1}. {labels[value]} ~ {prediction.values[index] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We will train the model for 10 epochs, with a batch size of 32. We will use the `ModelCheckpoint` callback to save the model after each epoch. We will also use the `EarlyStopping` callback to stop the training if the validation loss does not improve for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch 1 batch 0 loss 0.000803 [    0/50000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94891/3712570090.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float32)\n",
      "/tmp/ipykernel_94891/3712570090.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.target = torch.tensor(self.target, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 5000 loss 0.000450 [ 5000/50000]\n",
      "epoch 1 batch 10000 loss 0.001362 [10000/50000]\n",
      "epoch 1 batch 15000 loss 0.000693 [15000/50000]\n",
      "epoch 1 batch 20000 loss 0.000530 [20000/50000]\n",
      "epoch 1 batch 25000 loss 0.000216 [25000/50000]\n",
      "epoch 1 batch 30000 loss 0.000213 [30000/50000]\n",
      "epoch 1 batch 35000 loss 0.000456 [35000/50000]\n",
      "epoch 1 batch 40000 loss 0.000983 [40000/50000]\n",
      "epoch 1 batch 45000 loss 0.000343 [45000/50000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch 2 batch 0 loss 0.000525 [    0/50000]\n",
      "epoch 2 batch 5000 loss 0.000310 [ 5000/50000]\n",
      "epoch 2 batch 10000 loss 0.000939 [10000/50000]\n"
     ]
    }
   ],
   "source": [
    "def train(dataset, model, loss_fn, optimizer, num_epochs=10):\n",
    "    size = len(dataset)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        for batch, cube in enumerate(dataset):\n",
    "            # Transform cube data and target to tensors\n",
    "            cube.to_torch()\n",
    "            \n",
    "            # Compute prediction and loss\n",
    "            pred = model(cube.data.cuda())\n",
    "            loss = loss_fn(pred, cube.target.cuda())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 5000 == 0:\n",
    "                loss, _ = loss.item(), batch * len(cube.data)\n",
    "                print(f\"epoch {epoch+1} batch {batch} loss {loss:>7f} [{batch:>5d}/{size:>5d}]\")\n",
    "    \n",
    "\n",
    "training_dataset = dataset[:50000]\n",
    "testing_dataset = dataset[50000:]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=1e-3)\n",
    "\n",
    "train(training_dataset, model_1, loss_fn, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing : [    0/20000]\n",
      "testing : [ 6000/20000]\n",
      "testing : [12000/20000]\n",
      "testing : [18000/20000]\n",
      "testing : [24000/20000]\n",
      "testing : [30000/20000]\n",
      "testing : [36000/20000]\n",
      "testing : [42000/20000]\n",
      "testing : [48000/20000]\n",
      "testing : [54000/20000]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.001363 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(dataset, model, loss_fn):\n",
    "    size = len(dataset)\n",
    "    num_batches = len(dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, cube in enumerate(dataset):\n",
    "            cube.to_torch()\n",
    "\n",
    "            pred = model(cube.data.cuda())\n",
    "            test_loss += loss_fn(pred, cube.target.cuda()).item()\n",
    "\n",
    "            correct += (pred.argmax(0) == cube.target.argmax(0)).item()\n",
    "\n",
    "            if batch % 2000 == 0:\n",
    "                loss, current = test_loss / (batch + 1), batch * len(cube.data)\n",
    "                print(f\"testing : [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test(testing_dataset, model_1, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with air cubes\n",
    "\n",
    "Most cubes have some air blocks, so let's see how the model performs on cubes with way more air blocks than other blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 20, 1: 7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomCube(block_index=1, air_frequency=0.75).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model gave us a 100% accuracy on the testing set. This is expected as the model is very simple and the task is very simple. We will now move on to more complex models.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
