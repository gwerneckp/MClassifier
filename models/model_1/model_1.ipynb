{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "We first import `nbtschematic` to be able to load `.schematic` files. It uses `nbtlib` to parse the nbt file and some classes inherented from `numpy` classes to store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../schematics/apple.schematic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnbtschematic\u001b[39;00m \u001b[39mimport\u001b[39;00m SchematicFile\n\u001b[0;32m----> 2\u001b[0m sf \u001b[39m=\u001b[39m SchematicFile\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../schematics/apple.schematic\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m sf\n",
      "File \u001b[0;32m~/Projects/MClassifier/venv/lib/python3.11/site-packages/nbtschematic/schematic.py:159\u001b[0m, in \u001b[0;36mSchematicFile.load\u001b[0;34m(cls, filename, gzipped, byteorder)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, filename, gzipped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, byteorder\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbig\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSchematicFile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    146\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    Load a schematic file from disk\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m    :return: Loaded schematic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload(filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    160\u001b[0m                         gzipped\u001b[39m=\u001b[39;49mgzipped, byteorder\u001b[39m=\u001b[39;49mbyteorder)\n",
      "File \u001b[0;32m~/Projects/MClassifier/venv/lib/python3.11/site-packages/nbtlib/nbt.py:315\u001b[0m, in \u001b[0;36mFile.load\u001b[0;34m(cls, filename, gzipped, byteorder)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read, parse and return the nbt file at the specified location.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[39mThe method is used by the :func:`load` helper function when the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    byteorder: Can be either ``\"big\"`` or ``\"little\"``.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m open_file \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mopen \u001b[39mif\u001b[39;00m gzipped \u001b[39melse\u001b[39;00m \u001b[39mopen\u001b[39m\n\u001b[0;32m--> 315\u001b[0m \u001b[39mwith\u001b[39;00m open_file(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m fileobj:\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_fileobj(fileobj, byteorder)\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filename, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m, os\u001b[39m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[39m=\u001b[39m GzipFile(filename, gz_mode, compresslevel)\n\u001b[1;32m     59\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwrite\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[39m=\u001b[39m GzipFile(\u001b[39mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m fileobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmyfileobj \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, mode \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fileobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../schematics/apple.schematic'"
     ]
    }
   ],
   "source": [
    "from nbtschematic import SchematicFile\n",
    "sf = SchematicFile.load(\"../schematics/apple.schematic\")\n",
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "What matters to us is the `blocks` property of the `SchematicFile` object. It is a 3D array of `Block` objects. Each `Block` object is a Byte type holding the block id. To be able to use it in our model, we need to convert it into a numpy `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, -97,   0],\n",
       "        [-97, -97, -97],\n",
       "        [  0, -97,   0]],\n",
       "\n",
       "       [[-97, -97, -97],\n",
       "        [-97, -97, -97],\n",
       "        [-97, -97, -97]],\n",
       "\n",
       "       [[  0, -97,   0],\n",
       "        [-97, -97, -97],\n",
       "        [  0, -97,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,  35,   0],\n",
       "        [  0,   0,   0]]], dtype=int8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.asarray(sf.blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - cube with 9^3 blocks and 16 block types\n",
    "\n",
    "The first model is a simple cube with 9^3 blocks and 16 block types. \n",
    "\n",
    "**Objective:** Get the model to classify the blocks correctly, we will only provide schematics of cubes with ONLY 1 block type, but in random positions (We will basically fill the cube with 1 block type, and then remove some blocks randomly).\n",
    "\n",
    "**Why:** This is to make sure we are able to train the model to classify the blocks correctly, then we will move on to more complex models where it will actually be classifying multiple block structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid blocks\n",
    "\n",
    "We will only be using the following blocks:\n",
    "**Air, Dirt, Oak Log, Oak Leaves, Stone Brick, Cobblestone, Glass, Sandstone, Redstone Lamp, Iron Bars, Stone Brick, Bricks, Block of Quartz, White Wool, Bookshelf, White Terracotta, Nether Brick**\n",
    "\n",
    "\n",
    "I already have a schematic file with all these blocks lined up in a row, so we will just load that and use it to get the block ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5f758\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5f758_level0_col0\" class=\"col_heading level0 col0\" >Label</th>\n",
       "      <th id=\"T_5f758_level0_col1\" class=\"col_heading level0 col1\" >Block ID</th>\n",
       "      <th id=\"T_5f758_level0_col2\" class=\"col_heading level0 col2\" >Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row0_col0\" class=\"data row0 col0\" >Air</td>\n",
       "      <td id=\"T_5f758_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_5f758_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row1_col0\" class=\"data row1 col0\" >Dirt</td>\n",
       "      <td id=\"T_5f758_row1_col1\" class=\"data row1 col1\" >3</td>\n",
       "      <td id=\"T_5f758_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row2_col0\" class=\"data row2 col0\" >Oak Log</td>\n",
       "      <td id=\"T_5f758_row2_col1\" class=\"data row2 col1\" >17</td>\n",
       "      <td id=\"T_5f758_row2_col2\" class=\"data row2 col2\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row3_col0\" class=\"data row3 col0\" >Oak Leaves</td>\n",
       "      <td id=\"T_5f758_row3_col1\" class=\"data row3 col1\" >18</td>\n",
       "      <td id=\"T_5f758_row3_col2\" class=\"data row3 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row4_col0\" class=\"data row4 col0\" >Stone Brick</td>\n",
       "      <td id=\"T_5f758_row4_col1\" class=\"data row4 col1\" >97</td>\n",
       "      <td id=\"T_5f758_row4_col2\" class=\"data row4 col2\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row5_col0\" class=\"data row5 col0\" >Cobblestone</td>\n",
       "      <td id=\"T_5f758_row5_col1\" class=\"data row5 col1\" >4</td>\n",
       "      <td id=\"T_5f758_row5_col2\" class=\"data row5 col2\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row6_col0\" class=\"data row6 col0\" >Glass</td>\n",
       "      <td id=\"T_5f758_row6_col1\" class=\"data row6 col1\" >20</td>\n",
       "      <td id=\"T_5f758_row6_col2\" class=\"data row6 col2\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row7_col0\" class=\"data row7 col0\" >Sandstone</td>\n",
       "      <td id=\"T_5f758_row7_col1\" class=\"data row7 col1\" >24</td>\n",
       "      <td id=\"T_5f758_row7_col2\" class=\"data row7 col2\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row8_col0\" class=\"data row8 col0\" >Redstone Lamp</td>\n",
       "      <td id=\"T_5f758_row8_col1\" class=\"data row8 col1\" >123</td>\n",
       "      <td id=\"T_5f758_row8_col2\" class=\"data row8 col2\" >8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row9_col0\" class=\"data row9 col0\" >Iron Bars</td>\n",
       "      <td id=\"T_5f758_row9_col1\" class=\"data row9 col1\" >101</td>\n",
       "      <td id=\"T_5f758_row9_col2\" class=\"data row9 col2\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row10_col0\" class=\"data row10 col0\" >Bricks</td>\n",
       "      <td id=\"T_5f758_row10_col1\" class=\"data row10 col1\" >45</td>\n",
       "      <td id=\"T_5f758_row10_col2\" class=\"data row10 col2\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row11_col0\" class=\"data row11 col0\" >Block of Quartz</td>\n",
       "      <td id=\"T_5f758_row11_col1\" class=\"data row11 col1\" >-101</td>\n",
       "      <td id=\"T_5f758_row11_col2\" class=\"data row11 col2\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row12_col0\" class=\"data row12 col0\" >White Wool</td>\n",
       "      <td id=\"T_5f758_row12_col1\" class=\"data row12 col1\" >35</td>\n",
       "      <td id=\"T_5f758_row12_col2\" class=\"data row12 col2\" >12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row13_col0\" class=\"data row13 col0\" >Bookshelf</td>\n",
       "      <td id=\"T_5f758_row13_col1\" class=\"data row13 col1\" >47</td>\n",
       "      <td id=\"T_5f758_row13_col2\" class=\"data row13 col2\" >13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row14_col0\" class=\"data row14 col0\" >White Terracotta</td>\n",
       "      <td id=\"T_5f758_row14_col1\" class=\"data row14 col1\" >-97</td>\n",
       "      <td id=\"T_5f758_row14_col2\" class=\"data row14 col2\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5f758_row15_col0\" class=\"data row15 col0\" >Nether Brick</td>\n",
       "      <td id=\"T_5f758_row15_col1\" class=\"data row15 col1\" >112</td>\n",
       "      <td id=\"T_5f758_row15_col2\" class=\"data row15 col2\" >15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f603af58610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_blocks_schematic = SchematicFile.load(\"schematics/allblocks.schematic\")\n",
    "\n",
    "# We reverse the array because I want the air block to be in the front, it doesn't really matter though\n",
    "blocks = np.asarray(all_blocks_schematic.blocks).flatten()[::-1]\n",
    "\n",
    "# Labels are in the same order as the blocks\n",
    "labels = np.array(['Air', 'Dirt', 'Oak Log', 'Oak Leaves', 'Stone Brick', 'Cobblestone', 'Glass', 'Sandstone', 'Redstone Lamp', 'Iron Bars', 'Bricks', 'Block of Quartz', 'White Wool', 'Bookshelf', 'White Terracotta', 'Nether Brick'])\n",
    "\n",
    "# Create a table with the labels (block names), block ids and index in the array\n",
    "table = np.array([labels, blocks, np.arange(len(labels))]).T\n",
    "\n",
    "# print the table using pandas\n",
    "pd.DataFrame(table, columns=['Label', 'Block ID', 'Index']).style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding the blocks\n",
    "\n",
    "We will use a one hot encoding to represent the blocks. We will have a 16 length vector, with each index representing a block type. The index of the block type will be set to 1, and the rest will be 0.\n",
    "\n",
    "**Why:** Block types is a categorical variable, and we need to represent it in a way that the model can understand. One hot encoding is a good way to do this. If we were to use a simple integer encoding, the model would think that the block types are ordinal, and that the block type with the highest integer is the best block type. This is not the case, so we use one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the blocks to one hot encoding\n",
    "\n",
    "We will use the `block_ids` array to convert the blocks to one hot encoding. We will define a function `one_hot` to create a one hot encoded array and a `convert_blocks_to_one_hot` that takes an id an a dictionnary mapping the block ids to the one hot encoding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(lenght: int, index: int)->np.array:\n",
    "    \"\"\"\n",
    "    Creates a one hot array of the given lenght and sets the index to 1\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((lenght))\n",
    "    one_hot[index] = 1\n",
    "    return one_hot\n",
    "\n",
    "def convert_blocks_to_one_hot(block_id: int, possible_blocks: dict)->np.array:\n",
    "    \"\"\"\n",
    "    Converts an array of block ids to a one hot array\n",
    "    \"\"\"\n",
    "    return one_hot(len(possible_blocks), possible_blocks.get(block_id) or 0)\n",
    "\n",
    "# We need to convert the block ids array to a dictionary of block ids and their index in the array\n",
    "blocks_dict:dict = {block_id: index for index, block_id in enumerate(blocks)}\n",
    "\n",
    "# convert_blocks_to_one_hot(3, blocks_dict)\n",
    "one_hot(16, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "For other models, the dataset will be created inside Minecraft and exported as a `.schematic` file. For this model, we will create the dataset in python. We will randomly set blocks in the cube to a block type with the `random` module. Remember, in this model, **we are only training the model to classify the blocks correctly**, so we will only use 1 block type per cube. This should be a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomCube{count={1: 16, 0: 11}, data=[[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
       "\n",
       "\n",
       " [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
       "\n",
       "\n",
       " [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "\n",
       "  [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], target=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "dataset = []\n",
    "\n",
    "class RandomCube:\n",
    "    def __init__(self, block_index: int, dimensions: int=3, air_frequency=0.25):\n",
    "        self.block_index = block_index\n",
    "        self.dimensions = dimensions\n",
    "        self.air_frequency = air_frequency\n",
    "        self.data:np.array = []\n",
    "        self.target:np.array = []\n",
    "\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "        for x in range(self.dimensions):\n",
    "            self.data.append([])\n",
    "            for y in range(self.dimensions):\n",
    "                self.data[x].append([])\n",
    "                for z in range(self.dimensions):\n",
    "                    if random() > self.air_frequency:\n",
    "                        self.data[x][y].append(one_hot(16, self.block_index))\n",
    "                    else:\n",
    "                        self.data[x][y].append(one_hot(16, 0))\n",
    "\n",
    "        self.data = np.array(self.data)\n",
    "        self.target = one_hot(16, self.block_index)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def count(self: int)->int:\n",
    "        # Count each block and return a dict\n",
    "        block_count = {}\n",
    "\n",
    "        for x in self.data:\n",
    "            for y in x:\n",
    "               for z in y:\n",
    "                   for i, is_there in enumerate(z):\n",
    "                        if is_there == 1:\n",
    "                            block_count[i] = block_count.get(i, 0) + 1\n",
    "\n",
    "        return block_count\n",
    "                           \n",
    "    def __repr__(self):\n",
    "        return f\"RandomCube{{count={self.count}, data={self.data}, target={self.target}}}\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"RandomCube{{count={self.count}, data={self.data}, target={self.target}}}\"\n",
    "    \n",
    "    def to_torch(self):\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.target = torch.tensor(self.target, dtype=torch.float32)\n",
    "\n",
    "        return self\n",
    "\n",
    "RandomCube(block_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this RandomCube class, we can create a dataset about 70000 cubes. We will use 50000 for training, 20000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 70000 cubes, each cube has a size of (3, 3, 3, 16) and a target of (16,)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def generate_dataset(size: int=1000, dimensions: int=3, air_frequency: float=0.25)->List[RandomCube]:\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(size):\n",
    "        dataset.append(RandomCube(block_index=randint(0, 15), dimensions=dimensions, air_frequency=air_frequency))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = generate_dataset(size=70000, dimensions=3, air_frequency=0.25)\n",
    "print(f\"Generated {len(dataset)} cubes, each cube has a size of {dataset[0].data.shape} and a target of {dataset[0].target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "It is a 4D model as we have 4 dimensions: x, y, z, and block type. For this simple model, we will use a simple NN with 2 hidden layers. We will use the `relu` activation function for the hidden layers, and `softmax` for the output layer. We will use the `categorical_crossentropy` loss function, and the `adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BlockClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(432, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.flatten()\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the blocks\n",
    "\n",
    "Let's create a random tensor and pass it through the model to see what it predicts. It will give a random result as the model is not trained yet. The weights are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. White Wool ~ 20.86%\n",
      "2. Cobblestone ~ 14.97%\n",
      "3. Oak Log ~ 8.83%\n",
      "4. Redstone Lamp ~ 5.67%\n",
      "5. Glass ~ 1.76%\n",
      "6. Nether Brick ~ 0.96%\n",
      "7. Iron Bars ~ -0.04%\n",
      "8. Bricks ~ -0.54%\n",
      "9. Air ~ -0.63%\n",
      "10. Stone Brick ~ -2.66%\n",
      "11. Sandstone ~ -5.03%\n",
      "12. White Terracotta ~ -7.07%\n",
      "13. Dirt ~ -7.53%\n",
      "14. Block of Quartz ~ -9.33%\n",
      "15. Bookshelf ~ -9.62%\n",
      "16. Oak Leaves ~ -17.34%\n"
     ]
    }
   ],
   "source": [
    "model_1 = BlockClassifier().to(\"cuda\")\n",
    "random_prediction = torch.rand(3,3,3,16)\n",
    "prediction = model_1(random_prediction.cuda())\n",
    "\n",
    "prediction = torch.sort(prediction, descending=True)\n",
    "\n",
    "for index, value in enumerate(prediction.indices):\n",
    "    print(f\"{index+1}. {labels[value]} ~ {prediction.values[index] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We will train the model for 10 epochs, with a batch size of 32. We will use the `ModelCheckpoint` callback to save the model after each epoch. We will also use the `EarlyStopping` callback to stop the training if the validation loss does not improve for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch 1 batch 0 loss  0.000000 [    0/50000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97121/3712570090.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float32)\n",
      "/tmp/ipykernel_97121/3712570090.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.target = torch.tensor(self.target, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 5000 loss -0.000000 [ 5000/50000]\n",
      "epoch 1 batch 10000 loss  0.000000 [10000/50000]\n",
      "epoch 1 batch 15000 loss  0.000001 [15000/50000]\n",
      "epoch 1 batch 20000 loss  0.000000 [20000/50000]\n",
      "epoch 1 batch 25000 loss  0.000460 [25000/50000]\n",
      "epoch 1 batch 30000 loss  0.000001 [30000/50000]\n",
      "epoch 1 batch 35000 loss  0.000000 [35000/50000]\n",
      "epoch 1 batch 40000 loss -0.000000 [40000/50000]\n",
      "epoch 1 batch 45000 loss  0.000361 [45000/50000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch 2 batch 0 loss  0.000000 [    0/50000]\n",
      "epoch 2 batch 5000 loss -0.000000 [ 5000/50000]\n",
      "epoch 2 batch 10000 loss  0.000000 [10000/50000]\n",
      "epoch 2 batch 15000 loss  0.000001 [15000/50000]\n",
      "epoch 2 batch 20000 loss  0.000000 [20000/50000]\n",
      "epoch 2 batch 25000 loss  0.000275 [25000/50000]\n",
      "epoch 2 batch 30000 loss  0.000001 [30000/50000]\n",
      "epoch 2 batch 35000 loss  0.000000 [35000/50000]\n",
      "epoch 2 batch 40000 loss -0.000000 [40000/50000]\n",
      "epoch 2 batch 45000 loss  0.000236 [45000/50000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch 3 batch 0 loss  0.000000 [    0/50000]\n",
      "epoch 3 batch 5000 loss -0.000000 [ 5000/50000]\n",
      "epoch 3 batch 10000 loss  0.000000 [10000/50000]\n",
      "epoch 3 batch 15000 loss  0.000001 [15000/50000]\n",
      "epoch 3 batch 20000 loss  0.000000 [20000/50000]\n",
      "epoch 3 batch 25000 loss  0.000196 [25000/50000]\n",
      "epoch 3 batch 30000 loss  0.000001 [30000/50000]\n",
      "epoch 3 batch 35000 loss  0.000000 [35000/50000]\n",
      "epoch 3 batch 40000 loss -0.000000 [40000/50000]\n",
      "epoch 3 batch 45000 loss  0.000175 [45000/50000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def train(dataset, model, loss_fn, optimizer, num_epochs=10):\n",
    "    size = len(dataset)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        for batch, cube in enumerate(dataset):\n",
    "            # Transform cube data and target to tensors\n",
    "            cube.to_torch()\n",
    "            \n",
    "            # Compute prediction and loss\n",
    "            pred = model(cube.data.cuda())\n",
    "            loss = loss_fn(pred, cube.target.cuda())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 5000 == 0:\n",
    "                loss, _ = loss.item(), batch * len(cube.data)\n",
    "                print(f\"epoch {epoch+1} batch {batch} loss {loss:>9f} [{batch:>5d}/{size:>5d}]\")\n",
    "    \n",
    "\n",
    "training_dataset = dataset[:50000]\n",
    "testing_dataset = dataset[50000:]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=1e-3)\n",
    "\n",
    "train(training_dataset, model_1, loss_fn, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97121/3712570090.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float32)\n",
      "/tmp/ipykernel_97121/3712570090.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.target = torch.tensor(self.target, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing : [    0/20000]\n",
      "testing : [ 2000/20000]\n",
      "testing : [ 4000/20000]\n",
      "testing : [ 6000/20000]\n",
      "testing : [ 8000/20000]\n",
      "testing : [10000/20000]\n",
      "testing : [12000/20000]\n",
      "testing : [14000/20000]\n",
      "testing : [16000/20000]\n",
      "testing : [18000/20000]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000011 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(dataset, model, loss_fn):\n",
    "    size = len(dataset)\n",
    "    num_batches = len(dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, cube in enumerate(dataset):\n",
    "            cube.to_torch()\n",
    "\n",
    "            pred = model(cube.data.cuda())\n",
    "            test_loss += loss_fn(pred, cube.target.cuda()).item()\n",
    "\n",
    "            correct += (pred.argmax(0) == cube.target.argmax(0)).item()\n",
    "\n",
    "            \n",
    "            if batch % (size / 10) == 0:\n",
    "                loss, _ = test_loss / (batch + 1), batch * len(cube.data)\n",
    "                print(f\"testing : [{batch:>5d}/{size:>5d}]\")\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test(testing_dataset, model_1, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with air cubes\n",
    "\n",
    "Most cubes have some air blocks, so let's see how the model performs on cubes with way more air blocks than other blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing against 2344 cubes with 50% air\n",
      "testing : [    0/ 2344]\n",
      "Test Error: \n",
      " Accuracy: 99.7%, Avg loss: 0.011502 \n",
      "\n",
      "Testing against 2335 cubes with 75% air\n",
      "testing : [    0/ 2335]\n",
      "testing : [  467/ 2335]\n",
      "testing : [  934/ 2335]\n",
      "testing : [ 1401/ 2335]\n",
      "testing : [ 1868/ 2335]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.365967 \n",
      "\n",
      "Testing against 2343 cubes with 90% air\n",
      "testing : [    0/ 2343]\n",
      "Test Error: \n",
      " Accuracy: 2.3%, Avg loss: 6.996847 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_plain_air_blocks(cubes: List[RandomCube])->List[RandomCube]:\n",
    "    return list(filter(lambda cube: cube.target[0] == 0, cubes))\n",
    "\n",
    "cubes_50_percent_air = remove_plain_air_blocks(generate_dataset(size=2500, dimensions=3, air_frequency=0.5))\n",
    "\n",
    "print(f\"Testing against {len(cubes_50_percent_air)} cubes with 50% air\")\n",
    "test(cubes_50_percent_air, model_1, loss_fn)\n",
    "\n",
    "cubes_75_percent_air = remove_plain_air_blocks(generate_dataset(size=2500, dimensions=3, air_frequency=0.75))\n",
    "\n",
    "print(f\"Testing against {len(cubes_75_percent_air)} cubes with 75% air\")\n",
    "test(cubes_75_percent_air, model_1, loss_fn)\n",
    "\n",
    "cubes_90_percent_air = remove_plain_air_blocks(generate_dataset(size=2500, dimensions=3, air_frequency=0.9))\n",
    "\n",
    "print(f\"Testing against {len(cubes_90_percent_air)} cubes with 90% air\")\n",
    "test(cubes_90_percent_air, model_1, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on cubes with air blocks\n",
    "\n",
    "As we can see, the model performs poorly on cubes with a lot of air blocks. This is because the model is trained on cubes with only 1 block type and 25% of the blocks are air blocks. We will now train the model on cubes with more than that. The hope is that the neurons containing the air block information will be less active, and the model will be able to classify the other blocks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch 1 batch 0 loss 0.113964 [    0/ 2339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97121/3712570090.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float32)\n",
      "/tmp/ipykernel_97121/3712570090.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.target = torch.tensor(self.target, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch 2 batch 0 loss 0.021022 [    0/ 2339]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch 3 batch 0 loss 0.018804 [    0/ 2339]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch 1 batch 0 loss 0.546870 [    0/ 2345]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch 2 batch 0 loss 0.395194 [    0/ 2345]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch 3 batch 0 loss 0.253712 [    0/ 2345]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch 1 batch 0 loss 0.421239 [    0/ 2338]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch 2 batch 0 loss 0.192898 [    0/ 2338]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch 3 batch 0 loss 0.117331 [    0/ 2338]\n"
     ]
    }
   ],
   "source": [
    "train(cubes_50_percent_air, model_1, loss_fn, optimizer, num_epochs=3)\n",
    "train(cubes_75_percent_air, model_1, loss_fn, optimizer, num_epochs=3)\n",
    "train(cubes_90_percent_air, model_1, loss_fn, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "After the fine tuning, the model performs much better on cubes with a lot of air blocks. It is still not perfect, but it is much better than before. However, it performs worse on cubes with less air blocks. We can fine tune by training a bit more on cubes with less air blocks, as to get positive results on both types of cubes. However, we will move on to more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "We will save the model to a `.pth` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"models/model_1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
